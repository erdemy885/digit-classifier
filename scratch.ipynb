{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This is a sample Notebook to demonstrate how to read \"MNIST Dataset\"\n",
    "#\n",
    "import numpy as np # linear algebra\n",
    "import struct\n",
    "from array import array\n",
    "from os.path  import join\n",
    "\n",
    "#\n",
    "# MNIST Data Loader Class\n",
    "#\n",
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath,training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())        \n",
    "        \n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())        \n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images[i][:] = img            \n",
    "        \n",
    "        return images, labels\n",
    "            \n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (x_train, y_train),(x_test, y_test)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Verify Reading Dataset via MnistDataloader class\n",
    "#\n",
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#\n",
    "# Set file paths based on added MNIST Datasets\n",
    "#\n",
    "input_path = './data'\n",
    "training_images_filepath = join(input_path, 'train-images.idx3-ubyte')\n",
    "training_labels_filepath = join(input_path, 'train-labels.idx1-ubyte')\n",
    "test_images_filepath = join(input_path, 't10k-images.idx3-ubyte')\n",
    "test_labels_filepath = join(input_path, 't10k-labels.idx1-ubyte')\n",
    "\n",
    "#\n",
    "# Helper function to show a list of images with their relating titles\n",
    "#\n",
    "def show_images(images, title_texts):\n",
    "    cols = 5\n",
    "    rows = int(len(images)/cols) + 1\n",
    "    plt.figure(figsize=(30,20))\n",
    "    index = 1    \n",
    "    for x in zip(images, title_texts):        \n",
    "        image = x[0]        \n",
    "        title_text = x[1]\n",
    "        plt.subplot(rows, cols, index)        \n",
    "        plt.imshow(image, cmap=plt.cm.gray)\n",
    "        if (title_text != ''):\n",
    "            plt.title(title_text, fontsize = 15);        \n",
    "        index += 1\n",
    "\n",
    "#\n",
    "# Load MINST dataset\n",
    "#\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n",
    "\n",
    "#\n",
    "# Show some random training and test images \n",
    "#\n",
    "# images_2_show = []\n",
    "# titles_2_show = []\n",
    "# for i in range(0, 10):\n",
    "#     r = random.randint(1, 60000)\n",
    "#     images_2_show.append(x_train[r])\n",
    "#     titles_2_show.append('training image [' + str(r) + '] = ' + str(y_train[r]))    \n",
    "\n",
    "# for i in range(0, 5):\n",
    "#     r = random.randint(1, 10000)\n",
    "#     images_2_show.append(x_test[r])        \n",
    "#     titles_2_show.append('test image [' + str(r) + '] = ' + str(y_test[r]))    \n",
    "\n",
    "# show_images(images_2_show, titles_2_show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim, initialization=\"xavier\", activation=\"sigmoid\"):\n",
    "        \"\"\"\n",
    "        Initialize a neural network with the given dimensions and initialization method.\n",
    "\n",
    "        Parameters:\n",
    "            input_dim (int): Number of input features.\n",
    "            hidden_layers (list of int): List containing the number of units in each hidden layer.\n",
    "            output_dim (int): Number of output units.\n",
    "            initialization (str): Initialization method ('xavier', 'he', 'lecun', 'uniform_xavier', 'uniform_he', or 'uniform_lecun').\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.initialization = initialization\n",
    "        self.activation = activation\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        # Determine initialization scaling factor\n",
    "        def init_weight(shape, fan_in, fan_out):\n",
    "            if self.initialization == \"xavier\":\n",
    "                return np.random.randn(*shape) * np.sqrt(2 / (fan_in + fan_out))\n",
    "            elif self.initialization == \"he\":\n",
    "                return np.random.randn(*shape) * np.sqrt(2 / fan_in)\n",
    "            elif self.initialization == \"lecun\":\n",
    "                return np.random.randn(*shape) * np.sqrt(1 / fan_in)\n",
    "            elif self.initialization == \"uniform_xavier\":\n",
    "                limit = np.sqrt(6 / (fan_in + fan_out))\n",
    "                return np.random.uniform(-limit, limit, size=shape)\n",
    "            elif self.initialization == \"uniform_he\":\n",
    "                limit = np.sqrt(6 / fan_in)\n",
    "                return np.random.uniform(-limit, limit, size=shape)\n",
    "            elif self.initialization == \"uniform_lecun\":\n",
    "                limit = np.sqrt(3 / fan_in)\n",
    "                return np.random.uniform(-limit, limit, size=shape)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported initialization method. Choose 'xavier', 'he', 'lecun', 'uniform_xavier', 'uniform_he', or 'uniform_lecun'.\")\n",
    "\n",
    "        # Input to first hidden layer\n",
    "        self.weights.append(init_weight((hidden_layers[0], input_dim), input_dim, hidden_layers[0]))\n",
    "        self.biases.append(np.zeros((hidden_layers[0], 1)))\n",
    "\n",
    "        # Between hidden layers\n",
    "        for i in range(1, len(hidden_layers)):\n",
    "            self.weights.append(init_weight((hidden_layers[i], hidden_layers[i-1]), hidden_layers[i-1], hidden_layers[i]))\n",
    "            self.biases.append(np.zeros((hidden_layers[i], 1)))\n",
    "\n",
    "        # Last hidden layer to output layer\n",
    "        self.weights.append(init_weight((output_dim, hidden_layers[-1]), hidden_layers[-1], output_dim))\n",
    "        self.biases.append(np.zeros((output_dim, 1)))\n",
    "\n",
    "    def activate(self, x):\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "    def activation_derivative(self, x):\n",
    "        if self.activation == \"sigmoid\":\n",
    "            sigmoid = self.activate(x)\n",
    "            return sigmoid * (1 - sigmoid)\n",
    "\n",
    "    def compute_output(self, input):\n",
    "        output = self.activate((self.weights[0] @ input) + self.biases[0])\n",
    "        for i in range(1, len(self.weights)):\n",
    "            output = self.activate((self.weights[i] @ output) + self.biases[i])\n",
    "        return output\n",
    "    \n",
    "    def get_input_layer(self, input):\n",
    "        input_layer = []\n",
    "        for i in range(len(input)):\n",
    "            for j in range(len(input[i])):\n",
    "                input_layer.append(input[i][j])\n",
    "        return np.array(input_layer).reshape(-1, 1)\n",
    "    \n",
    "    def cost(self, output, label):\n",
    "        target = np.zeros((self.output_dim, 1))\n",
    "        target[label] = 1\n",
    "        diff_squared = np.square(output - target)\n",
    "        cost = 0\n",
    "        for i in range(len(diff_squared)):\n",
    "            cost += diff_squared[i]\n",
    "        return cost\n",
    "    \n",
    "    def total_cost(self, input_data, input_labels):\n",
    "        sum = 0\n",
    "        for i in range(len(input_data)):\n",
    "            input_layer = self.get_input_layer(input_data[i])\n",
    "            output_layer = self.compute_output(input_layer)\n",
    "            sum += self.cost(output_layer, input_labels[i])\n",
    "        return sum/len(input_data)\n",
    "\n",
    "    def forward_pass(self, input):\n",
    "        activations = [input]\n",
    "        weighted_sums = []\n",
    "\n",
    "        ws = (self.weights[0] @ input) + self.biases[0]\n",
    "        act = self.activate(ws)\n",
    "\n",
    "        weighted_sums.append(ws)\n",
    "        activations.append(act)\n",
    "\n",
    "        for i in range(1, len(self.weights)):\n",
    "            ws = (self.weights[i] @ act) + self.biases[i]\n",
    "            act = self.activate(ws)\n",
    "\n",
    "            weighted_sums.append(ws)\n",
    "            activations.append(act)\n",
    "\n",
    "        return activations, weighted_sums\n",
    "        \n",
    "    \n",
    "    def compute_gradient(self, input, label):\n",
    "        activations, weighted_sums = self.forward_pass(input)\n",
    "\n",
    "        target = np.zeros((self.output_dim, 1))\n",
    "        target[label] = 1\n",
    "\n",
    "        bias_derivatives = []\n",
    "        weight_derivatives = []\n",
    "\n",
    "        db = self.activation_derivative(weighted_sums[-1]) * 2 * (activations[-1] - target)\n",
    "        dw = db @ np.transpose(activations[-2])\n",
    "        da = np.transpose(self.weights[-1]) @ db\n",
    "\n",
    "        bias_derivatives.insert(0, db)\n",
    "        weight_derivatives.insert(0, dw)\n",
    "\n",
    "        for i in range(2, len(self.weights) + 1):\n",
    "            db = self.activation_derivative(weighted_sums[-i]) * da\n",
    "            dw = db @ np.transpose(activations[-(i+1)])\n",
    "            da = np.transpose(self.weights[-i]) @ db\n",
    "\n",
    "            bias_derivatives.insert(0, db)\n",
    "            weight_derivatives.insert(0, dw)\n",
    "\n",
    "        return weight_derivatives, bias_derivatives\n",
    "\n",
    "    def stochastic_gradient(self, input_data, input_labels, sample_size):\n",
    "        sample = [random.randint(0, len(input_data) - 1) for _ in range(sample_size)]\n",
    "\n",
    "        weight_gradient, bias_gradient = self.compute_gradient(\n",
    "            self.get_input_layer(input_data[sample[0]]),\n",
    "            input_labels[sample[0]])\n",
    "        \n",
    "        for i in range(1, sample_size):\n",
    "            weight_derivatives, bias_derivatives = self.compute_gradient(\n",
    "                self.get_input_layer(input_data[sample[i]]),\n",
    "                input_labels[sample[i]]\n",
    "            )\n",
    "            for j in range(len(weight_gradient)):\n",
    "                weight_gradient[j] = weight_gradient[j] + weight_derivatives[j]\n",
    "                bias_gradient[j] = bias_gradient[j] + bias_derivatives[j]\n",
    "            \n",
    "        for i in range(len(weight_gradient)):\n",
    "            weight_gradient[i] = weight_gradient[i]/sample_size\n",
    "            bias_gradient[i] = bias_gradient[i]/sample_size\n",
    "        \n",
    "        return weight_gradient, bias_gradient\n",
    "\n",
    "    def batch_gradient(self, input_data, input_labels):\n",
    "        weight_gradient, bias_gradient = self.compute_gradient(\n",
    "            self.get_input_layer(input_data[0]),\n",
    "            input_labels[0])\n",
    "        \n",
    "        for i in range(1, len(input_data)):\n",
    "            weight_derivatives, bias_derivatives = self.compute_gradient(\n",
    "                self.get_input_layer(input_data[i]),\n",
    "                input_labels[i]\n",
    "            )\n",
    "            for j in range(len(weight_gradient)):\n",
    "                weight_gradient[j] = weight_gradient[j] + weight_derivatives[j]\n",
    "                bias_gradient[j] = bias_gradient[j] + bias_derivatives[j]\n",
    "            \n",
    "        for i in range(len(weight_gradient)):\n",
    "            weight_gradient[i] = weight_gradient[i]/len(input_data)\n",
    "            bias_gradient[i] = bias_gradient[i]/len(input_data)\n",
    "        \n",
    "        return weight_gradient, bias_gradient\n",
    "    \n",
    "    def stochastic_gradient_descent(self, input_data, input_labels, sample_size, learning_rate, threshold):\n",
    "        # to be implemented\n",
    "        return None\n",
    "\n",
    "    def batch_gradient_descent(self, input_data, input_labels, learning_rate, threshold):\n",
    "        # to be implemented\n",
    "        return None\n",
    "    \n",
    "    def test(self, test_data, test_labels):\n",
    "        # to be implemented\n",
    "        return None\n",
    "    \n",
    "    def make_inference(self, input):\n",
    "        # to be implemented\n",
    "        return None\n",
    "\n",
    "    def summarize(self):\n",
    "        \"\"\"Print a summary of the network's dimensions and parameter shapes.\"\"\"\n",
    "        print(\"Neural Network Summary:\")\n",
    "        print(f\"Input dimension: {self.input_dim}\")\n",
    "        print(f\"Hidden layers: {self.hidden_layers}\")\n",
    "        print(f\"Output dimension: {self.output_dim}\")\n",
    "        print(f\"Initialization method: {self.initialization}\\n\")\n",
    "        for idx, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
    "            print(f\"Layer {idx + 1} weights shape: {w.shape}\")\n",
    "            print(f\"Layer {idx + 1} biases shape: {b.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(784, [16, 16], 10, \"xavier\", \"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]),\n",
       "  array([[ 1.00413247e-94,  4.25898509e-23,  1.01448952e-46,\n",
       "           4.67762309e-02,  4.67762309e-02,  1.37388415e-68,\n",
       "           9.78465440e-83,  3.33274906e-15,  2.64860076e-52,\n",
       "           4.67762309e-02,  5.43570892e-14,  3.05527873e-34,\n",
       "           1.68074144e-54,  8.35367676e-77,  9.42148291e-85,\n",
       "           2.42373515e-04],\n",
       "         [ 7.33456531e-95,  3.11092462e-23,  7.41021710e-47,\n",
       "           3.41671374e-02,  3.41671374e-02,  1.00353721e-68,\n",
       "           7.14708356e-83,  2.43436662e-15,  1.93463869e-52,\n",
       "           3.41671374e-02,  3.97044844e-14,  2.23169174e-34,\n",
       "           1.22767744e-54,  6.10184308e-77,  6.88180929e-85,\n",
       "           1.77038830e-04],\n",
       "         [ 5.23212811e-95,  2.21918484e-23,  5.28609448e-47,\n",
       "           2.43732017e-02,  2.43732017e-02,  7.15875454e-69,\n",
       "           5.09838759e-83,  1.73656072e-15,  1.38007871e-52,\n",
       "           2.43732017e-02,  2.83232803e-14,  1.59198215e-34,\n",
       "           8.75766374e-55,  4.35276303e-77,  4.90915362e-85,\n",
       "           1.26291034e-04],\n",
       "         [-5.87613693e-95, -2.49233844e-23, -5.93674588e-47,\n",
       "          -2.73732347e-02, -2.73732347e-02, -8.03990671e-69,\n",
       "          -5.72593465e-83, -1.95030939e-15, -1.54994895e-52,\n",
       "          -2.73732347e-02, -3.18095180e-14, -1.78793503e-34,\n",
       "          -9.83562142e-55, -4.88853313e-77, -5.51340837e-85,\n",
       "          -1.41835864e-04],\n",
       "         [-1.55941019e-94, -6.61417188e-23, -1.57549460e-46,\n",
       "          -7.26431356e-02, -7.26431356e-02, -2.13363177e-68,\n",
       "          -1.51954949e-82, -5.17573429e-15, -4.11325710e-52,\n",
       "          -7.26431356e-02, -8.44161516e-14, -4.74482494e-34,\n",
       "          -2.61017884e-54, -1.29731973e-76, -1.46314923e-84,\n",
       "          -3.76404250e-04],\n",
       "         [-5.75821137e-95, -2.44232082e-23, -5.81760399e-47,\n",
       "          -2.68238935e-02, -2.68238935e-02, -7.87855742e-69,\n",
       "          -5.61102343e-83, -1.91116950e-15, -1.51884372e-52,\n",
       "          -2.68238935e-02, -3.11711470e-14, -1.75205376e-34,\n",
       "          -9.63823473e-55, -4.79042735e-77, -5.40276224e-85,\n",
       "          -1.38989423e-04],\n",
       "         [-1.15971395e-94, -4.91887732e-23, -1.17167573e-46,\n",
       "          -5.40237960e-02, -5.40237960e-02, -1.58675540e-68,\n",
       "          -1.13007004e-82, -3.84912918e-15, -3.05897812e-52,\n",
       "          -5.40237960e-02, -6.27792414e-14, -3.52866726e-34,\n",
       "          -1.94115752e-54, -9.64800540e-77, -1.08812587e-84,\n",
       "          -2.79927157e-04],\n",
       "         [-4.45868589e-95, -1.89113262e-23, -4.50467465e-47,\n",
       "          -2.07702198e-02, -2.07702198e-02, -6.10050770e-69,\n",
       "          -4.34471564e-83, -1.47985267e-15, -1.17606782e-52,\n",
       "          -2.07702198e-02, -2.41363758e-14, -1.35664651e-34,\n",
       "          -7.46305727e-55, -3.70931344e-77, -4.18345528e-85,\n",
       "          -1.07621992e-04],\n",
       "         [-2.20017690e-95, -9.33195656e-24, -2.22287045e-47,\n",
       "          -1.02492436e-02, -1.02492436e-02, -3.01034799e-69,\n",
       "          -2.14393730e-83, -7.30246029e-16, -5.80340778e-53,\n",
       "          -1.02492436e-02, -1.19103022e-14, -6.69448889e-35,\n",
       "          -3.68270979e-55, -1.83039262e-77, -2.06436199e-85,\n",
       "          -5.31069977e-05],\n",
       "         [-8.03264952e-95, -3.40701406e-23, -8.11550164e-47,\n",
       "          -3.74190736e-02, -3.74190736e-02, -1.09905119e-68,\n",
       "          -7.82732375e-83, -2.66606309e-15, -2.11877239e-52,\n",
       "          -3.74190736e-02, -4.34834505e-14, -2.44409815e-34,\n",
       "          -1.34452448e-54, -6.68260010e-77, -7.53680140e-85,\n",
       "          -1.93888909e-04],\n",
       "         [ 1.62472578e-94,  6.89120516e-23,  1.64148388e-46,\n",
       "           7.56857790e-02,  7.56857790e-02,  2.22299852e-68,\n",
       "           1.58319551e-82,  5.39251889e-15,  4.28554006e-52,\n",
       "           7.56857790e-02,  8.79519054e-14,  4.94356099e-34,\n",
       "           2.71950566e-54,  1.35165771e-76,  1.52443294e-84,\n",
       "           3.92169868e-04],\n",
       "         [-1.12754135e-94, -4.78241859e-23, -1.13917129e-46,\n",
       "          -5.25250762e-02, -5.25250762e-02, -1.54273588e-68,\n",
       "          -1.09871981e-82, -3.74234724e-15, -2.97411642e-52,\n",
       "          -5.25250762e-02, -6.10376295e-14, -3.43077552e-34,\n",
       "          -1.88730623e-54, -9.38035193e-77, -1.05793925e-84,\n",
       "          -2.72161461e-04],\n",
       "         [ 2.80654133e-95,  1.19038254e-23,  2.83548917e-47,\n",
       "           1.30739149e-02,  1.30739149e-02,  3.83999398e-69,\n",
       "           2.73480220e-83,  9.31500399e-16,  7.40281556e-53,\n",
       "           1.30739149e-02,  1.51927581e-14,  8.53947687e-35,\n",
       "           4.69765737e-55,  2.33484522e-77,  2.63329609e-85,\n",
       "           6.77431819e-05],\n",
       "         [ 1.35915051e-94,  5.76477898e-23,  1.37316936e-46,\n",
       "           6.33142938e-02,  6.33142938e-02,  1.85963048e-68,\n",
       "           1.32440872e-82,  4.51106575e-15,  3.58503204e-52,\n",
       "           6.33142938e-02,  7.35754173e-14,  4.13549384e-34,\n",
       "           2.27497930e-54,  1.13071775e-76,  1.27525139e-84,\n",
       "           3.28066363e-04],\n",
       "         [ 7.79292607e-95,  3.30533638e-23,  7.87330558e-47,\n",
       "           3.63023525e-02,  3.63023525e-02,  1.06625151e-68,\n",
       "           7.59372797e-83,  2.58649808e-15,  2.05554053e-52,\n",
       "           3.63023525e-02,  4.21857463e-14,  2.37115738e-34,\n",
       "           1.30439899e-54,  6.48316703e-77,  7.31187586e-85,\n",
       "           1.88102560e-04],\n",
       "         [ 6.66485051e-95,  2.82686794e-23,  6.73359458e-47,\n",
       "           3.10473564e-02,  3.10473564e-02,  9.11904827e-69,\n",
       "           6.49448760e-83,  2.21208605e-15,  1.75798798e-52,\n",
       "           3.10473564e-02,  3.60790915e-14,  2.02791729e-34,\n",
       "           1.11557895e-54,  5.54468742e-77,  6.25343537e-85,\n",
       "           1.60873519e-04]]),\n",
       "  array([[ 0.06256646,  0.06652383,  0.05296604,  0.09993851,  0.06564362,\n",
       "           0.07410368,  0.06133364,  0.06079576,  0.07123513,  0.06830217,\n",
       "           0.07796393,  0.05822203,  0.07653829,  0.0694596 ,  0.06971956,\n",
       "           0.05028362],\n",
       "         [ 0.14346153,  0.15253558,  0.1214483 ,  0.22915364,  0.1505173 ,\n",
       "           0.16991577,  0.14063474,  0.13940142,  0.16333834,  0.15661322,\n",
       "           0.17876711,  0.13349999,  0.1754982 ,  0.15926714,  0.15986322,\n",
       "           0.11529764],\n",
       "         [ 0.12242836,  0.13017204,  0.10364253,  0.19555698,  0.12844966,\n",
       "           0.14500409,  0.12001601,  0.11896351,  0.13939098,  0.13365185,\n",
       "           0.15255772,  0.1139273 ,  0.14976807,  0.13591667,  0.13642536,\n",
       "           0.09839363],\n",
       "         [ 0.0943973 ,  0.100368  ,  0.07991265,  0.15078248,  0.09903998,\n",
       "           0.11180412,  0.09253728,  0.09172576,  0.10747619,  0.10305108,\n",
       "           0.11762828,  0.08784264,  0.11547735,  0.10479735,  0.10518957,\n",
       "           0.07586554],\n",
       "         [ 0.12404469,  0.13189061,  0.10501084,  0.19813878,  0.13014549,\n",
       "           0.14691847,  0.12160049,  0.1205341 ,  0.14123127,  0.13541636,\n",
       "           0.15457183,  0.1154314 ,  0.15174535,  0.13771108,  0.13822649,\n",
       "           0.09969265],\n",
       "         [ 0.09686356,  0.10299025,  0.08200048,  0.15472189,  0.10162754,\n",
       "           0.11472516,  0.09495495,  0.09412223,  0.11028415,  0.10574343,\n",
       "           0.12070148,  0.09013765,  0.11849435,  0.10753533,  0.1079378 ,\n",
       "           0.07784763],\n",
       "         [ 0.11696248,  0.12436044,  0.09901535,  0.18682624,  0.12271496,\n",
       "           0.1385303 ,  0.11465783,  0.11365232,  0.1331678 ,  0.12768489,\n",
       "           0.1457467 ,  0.10884096,  0.14308159,  0.1298486 ,  0.13033458,\n",
       "           0.09400079],\n",
       "         [ 0.11285766,  0.11999599,  0.09554039,  0.18026954,  0.11840826,\n",
       "           0.13366856,  0.11063389,  0.10966367,  0.12849425,  0.12320377,\n",
       "           0.14063169,  0.10502116,  0.13806012,  0.12529154,  0.12576046,\n",
       "           0.09070182],\n",
       "         [ 0.11627981,  0.12363459,  0.09843744,  0.18573581,  0.12199872,\n",
       "           0.13772176,  0.11398862,  0.11298897,  0.13239055,  0.12693964,\n",
       "           0.14489603,  0.10820569,  0.14224648,  0.12909072,  0.12957387,\n",
       "           0.09345215],\n",
       "         [-0.13751651, -0.14621453, -0.1164155 , -0.21965756, -0.14427989,\n",
       "          -0.16287449, -0.13480687, -0.13362465, -0.15656963, -0.1501232 ,\n",
       "          -0.17135904, -0.12796778, -0.16822559, -0.15266714, -0.15323852,\n",
       "          -0.11051973]])],\n",
       " [array([[ 2.14981751e-95],\n",
       "         [ 2.09143483e-23],\n",
       "         [-1.47441265e-46],\n",
       "         [-0.00000000e+00],\n",
       "         [ 0.00000000e+00],\n",
       "         [ 3.16471448e-69],\n",
       "         [ 1.22313195e-82],\n",
       "         [-4.75438326e-15],\n",
       "         [-1.61240502e-52],\n",
       "         [-0.00000000e+00],\n",
       "         [-1.05009556e-13],\n",
       "         [ 2.05410489e-34],\n",
       "         [ 1.34537023e-54],\n",
       "         [ 8.05663922e-77],\n",
       "         [ 9.20218021e-86],\n",
       "         [-1.75342638e-04]]),\n",
       "  array([[ 0.04677623],\n",
       "         [ 0.03416714],\n",
       "         [ 0.0243732 ],\n",
       "         [-0.02737323],\n",
       "         [-0.07264314],\n",
       "         [-0.02682389],\n",
       "         [-0.0540238 ],\n",
       "         [-0.02077022],\n",
       "         [-0.01024924],\n",
       "         [-0.03741907],\n",
       "         [ 0.07568578],\n",
       "         [-0.05252508],\n",
       "         [ 0.01307391],\n",
       "         [ 0.06331429],\n",
       "         [ 0.03630235],\n",
       "         [ 0.03104736]]),\n",
       "  array([[ 0.12895575],\n",
       "         [ 0.29568862],\n",
       "         [ 0.25233714],\n",
       "         [ 0.19456232],\n",
       "         [ 0.25566857],\n",
       "         [ 0.19964554],\n",
       "         [ 0.24107141],\n",
       "         [ 0.23261097],\n",
       "         [ 0.23966438],\n",
       "         [-0.28343535]])])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.compute_gradient(nn.get_input_layer(x_train[4]), y_train[4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
