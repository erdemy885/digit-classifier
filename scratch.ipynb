{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This is a sample Notebook to demonstrate how to read \"MNIST Dataset\"\n",
    "#\n",
    "import numpy as np # linear algebra\n",
    "import struct\n",
    "from array import array\n",
    "from os.path  import join\n",
    "\n",
    "#\n",
    "# MNIST Data Loader Class\n",
    "#\n",
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath,training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())        \n",
    "        \n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())        \n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images[i][:] = img            \n",
    "        \n",
    "        return images, labels\n",
    "            \n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (x_train, y_train),(x_test, y_test)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Verify Reading Dataset via MnistDataloader class\n",
    "#\n",
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#\n",
    "# Set file paths based on added MNIST Datasets\n",
    "#\n",
    "input_path = './data'\n",
    "training_images_filepath = join(input_path, 'train-images.idx3-ubyte')\n",
    "training_labels_filepath = join(input_path, 'train-labels.idx1-ubyte')\n",
    "test_images_filepath = join(input_path, 't10k-images.idx3-ubyte')\n",
    "test_labels_filepath = join(input_path, 't10k-labels.idx1-ubyte')\n",
    "\n",
    "#\n",
    "# Helper function to show a list of images with their relating titles\n",
    "#\n",
    "def show_images(images, title_texts):\n",
    "    cols = 5\n",
    "    rows = int(len(images)/cols) + 1\n",
    "    plt.figure(figsize=(30,20))\n",
    "    index = 1    \n",
    "    for x in zip(images, title_texts):        \n",
    "        image = x[0]        \n",
    "        title_text = x[1]\n",
    "        plt.subplot(rows, cols, index)        \n",
    "        plt.imshow(image, cmap=plt.cm.gray)\n",
    "        if (title_text != ''):\n",
    "            plt.title(title_text, fontsize = 15);        \n",
    "        index += 1\n",
    "\n",
    "#\n",
    "# Load MINST dataset\n",
    "#\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n",
    "\n",
    "#\n",
    "# Show some random training and test images \n",
    "#\n",
    "# images_2_show = []\n",
    "# titles_2_show = []\n",
    "# for i in range(0, 10):\n",
    "#     r = random.randint(1, 60000)\n",
    "#     images_2_show.append(x_train[r])\n",
    "#     titles_2_show.append('training image [' + str(r) + '] = ' + str(y_train[r]))    \n",
    "\n",
    "# for i in range(0, 5):\n",
    "#     r = random.randint(1, 10000)\n",
    "#     images_2_show.append(x_test[r])        \n",
    "#     titles_2_show.append('test image [' + str(r) + '] = ' + str(y_test[r]))    \n",
    "\n",
    "# show_images(images_2_show, titles_2_show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "x_train, x_validate, y_train, y_validate = train_test_split(\n",
    "    x_train, y_train, test_size=(1/6), random_state=42\n",
    ")\n",
    "\n",
    "print(len(x_train))      # Output: 50000    5/6th of training data \n",
    "print(len(x_validate))   # Output: 10000    1/6th of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim, initialization=\"xavier\", activation=\"sigmoid\"):\n",
    "        \"\"\"\n",
    "        Initialize a neural network with the given dimensions and initialization method.\n",
    "\n",
    "        Parameters:\n",
    "            input_dim (int): Number of input features.\n",
    "            hidden_layers (list of int): List containing the number of units in each hidden layer.\n",
    "            output_dim (int): Number of output units.\n",
    "            initialization (str): Initialization method ('xavier', 'he', 'lecun', 'uniform_xavier', 'uniform_he', or 'uniform_lecun').\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.initialization = initialization\n",
    "        self.activation = activation\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        # Determine initialization scaling factor\n",
    "        def init_weight(shape, fan_in, fan_out):\n",
    "            return np.random.randn(*shape)\n",
    "            # if self.initialization == \"xavier\":\n",
    "            #     return np.random.randn(*shape) * np.sqrt(2 / (fan_in + fan_out))\n",
    "            # elif self.initialization == \"he\":\n",
    "            #     return np.random.randn(*shape) * np.sqrt(2 / fan_in)\n",
    "            # elif self.initialization == \"lecun\":\n",
    "            #     return np.random.randn(*shape) * np.sqrt(1 / fan_in)\n",
    "            # elif self.initialization == \"uniform_xavier\":\n",
    "            #     limit = np.sqrt(6 / (fan_in + fan_out))\n",
    "            #     return np.random.uniform(-limit, limit, size=shape)\n",
    "            # elif self.initialization == \"uniform_he\":\n",
    "            #     limit = np.sqrt(6 / fan_in)\n",
    "            #     return np.random.uniform(-limit, limit, size=shape)\n",
    "            # elif self.initialization == \"uniform_lecun\":\n",
    "            #     limit = np.sqrt(3 / fan_in)\n",
    "            #     return np.random.uniform(-limit, limit, size=shape)\n",
    "            # else:\n",
    "            #     raise ValueError(\"Unsupported initialization method. Choose 'xavier', 'he', 'lecun', 'uniform_xavier', 'uniform_he', or 'uniform_lecun'.\")\n",
    "\n",
    "        # Input to first hidden layer\n",
    "        self.weights.append(init_weight((hidden_layers[0], input_dim), input_dim, hidden_layers[0]))\n",
    "        self.biases.append(np.zeros((hidden_layers[0], 1)))\n",
    "\n",
    "        # Between hidden layers\n",
    "        for i in range(1, len(hidden_layers)):\n",
    "            self.weights.append(init_weight((hidden_layers[i], hidden_layers[i-1]), hidden_layers[i-1], hidden_layers[i]))\n",
    "            self.biases.append(np.zeros((hidden_layers[i], 1)))\n",
    "\n",
    "        # Last hidden layer to output layer\n",
    "        self.weights.append(init_weight((output_dim, hidden_layers[-1]), hidden_layers[-1], output_dim))\n",
    "        self.biases.append(np.zeros((output_dim, 1)))\n",
    "\n",
    "    def activate(self, x):\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "    def activation_derivative(self, x):\n",
    "        if self.activation == \"sigmoid\":\n",
    "            sigmoid = self.activate(x)\n",
    "            return sigmoid * (1 - sigmoid)\n",
    "\n",
    "    def compute_output(self, input):\n",
    "        output = self.activate((self.weights[0] @ input) + self.biases[0])\n",
    "        for i in range(1, len(self.weights)):\n",
    "            output = self.activate((self.weights[i] @ output) + self.biases[i])\n",
    "        return output\n",
    "    \n",
    "    def cost(self, output, label):\n",
    "        target = np.zeros((self.output_dim, 1))\n",
    "        target[label] = 1\n",
    "        diff_squared = np.square(output - target)\n",
    "        cost = 0\n",
    "        for i in range(len(diff_squared)):\n",
    "            cost += diff_squared[i]\n",
    "        return cost.item()\n",
    "    \n",
    "    def total_cost(self, input_data, input_labels):\n",
    "        sum = 0\n",
    "        for i in range(len(input_data)):\n",
    "            input_layer = self.get_input_layer(input_data[i])\n",
    "            output_layer = self.compute_output(input_layer)\n",
    "            sum += self.cost(output_layer, input_labels[i])\n",
    "        return sum/len(input_data)\n",
    "\n",
    "    def forward_pass(self, input):\n",
    "        activations = [input]\n",
    "        weighted_sums = []\n",
    "\n",
    "        ws = (self.weights[0] @ input) + self.biases[0]\n",
    "        act = self.activate(ws)\n",
    "\n",
    "        weighted_sums.append(ws)\n",
    "        activations.append(act)\n",
    "\n",
    "        for i in range(1, len(self.weights)):\n",
    "            ws = (self.weights[i] @ act) + self.biases[i]\n",
    "            act = self.activate(ws)\n",
    "\n",
    "            weighted_sums.append(ws)\n",
    "            activations.append(act)\n",
    "\n",
    "        return activations, weighted_sums\n",
    "        \n",
    "    \n",
    "    def compute_gradient(self, input, label):\n",
    "        activations, weighted_sums = self.forward_pass(input)\n",
    "\n",
    "        target = np.zeros((self.output_dim, 1))\n",
    "        target[label] = 1\n",
    "\n",
    "        bias_derivatives = []\n",
    "        weight_derivatives = []\n",
    "\n",
    "        db = self.activation_derivative(weighted_sums[-1]) * 2 * (activations[-1] - target)\n",
    "        dw = db @ np.transpose(activations[-2])\n",
    "        da = np.transpose(self.weights[-1]) @ db\n",
    "\n",
    "        bias_derivatives.insert(0, db)\n",
    "        weight_derivatives.insert(0, dw)\n",
    "\n",
    "        for i in range(2, len(self.weights) + 1):\n",
    "            db = self.activation_derivative(weighted_sums[-i]) * da\n",
    "            dw = db @ np.transpose(activations[-(i+1)])\n",
    "            da = np.transpose(self.weights[-i]) @ db\n",
    "\n",
    "            bias_derivatives.insert(0, db)\n",
    "            weight_derivatives.insert(0, dw)\n",
    "\n",
    "        return weight_derivatives, bias_derivatives\n",
    "\n",
    "    def stochastic_gradient(self, input_data, input_labels, sample_size):\n",
    "        sample = [random.randint(0, len(input_data) - 1) for _ in range(sample_size)]\n",
    "\n",
    "        weight_gradient, bias_gradient = self.compute_gradient(\n",
    "            self.get_input_layer(input_data[sample[0]]),\n",
    "            input_labels[sample[0]])\n",
    "        \n",
    "        for i in range(1, sample_size):\n",
    "            weight_derivatives, bias_derivatives = self.compute_gradient(\n",
    "                self.get_input_layer(input_data[sample[i]]),\n",
    "                input_labels[sample[i]]\n",
    "            )\n",
    "            for j in range(len(weight_gradient)):\n",
    "                weight_gradient[j] = weight_gradient[j] + weight_derivatives[j]\n",
    "                bias_gradient[j] = bias_gradient[j] + bias_derivatives[j]\n",
    "            \n",
    "        for i in range(len(weight_gradient)):\n",
    "            weight_gradient[i] = weight_gradient[i]/sample_size\n",
    "            bias_gradient[i] = bias_gradient[i]/sample_size\n",
    "        \n",
    "        return weight_gradient, bias_gradient\n",
    "\n",
    "    def batch_gradient(self, input_data, input_labels):\n",
    "        weight_gradient, bias_gradient = self.compute_gradient(\n",
    "            self.get_input_layer(input_data[0]),\n",
    "            input_labels[0])\n",
    "        \n",
    "        for i in range(1, len(input_data)):\n",
    "            weight_derivatives, bias_derivatives = self.compute_gradient(\n",
    "                self.get_input_layer(input_data[i]),\n",
    "                input_labels[i]\n",
    "            )\n",
    "            for j in range(len(weight_gradient)):\n",
    "                weight_gradient[j] = weight_gradient[j] + weight_derivatives[j]\n",
    "                bias_gradient[j] = bias_gradient[j] + bias_derivatives[j]\n",
    "            \n",
    "        for i in range(len(weight_gradient)):\n",
    "            weight_gradient[i] = weight_gradient[i]/len(input_data)\n",
    "            bias_gradient[i] = bias_gradient[i]/len(input_data)\n",
    "        \n",
    "        return weight_gradient, bias_gradient\n",
    "    \n",
    "    def stochastic_gradient_descent(self, input_data, input_labels, sample_size, learning_rate, iterations):\n",
    "        for _ in range(iterations):\n",
    "            weight_gradient, bias_gradient = self.stochastic_gradient(input_data, input_labels, sample_size)\n",
    "            for i in range(len(self.weights)):\n",
    "                self.weights[i] = self.weights[i] - learning_rate * weight_gradient[i]\n",
    "                self.biases[i] = self.biases[i] - learning_rate * bias_gradient[i]\n",
    "\n",
    "    def batch_gradient_descent(self, input_data, input_labels, learning_rate, iterations):\n",
    "        for _ in range(iterations):\n",
    "            weight_gradient, bias_gradient = self.batch_gradient(input_data, input_labels)\n",
    "            for i in range(len(self.weights)):\n",
    "                self.weights[i] = self.weights[i] - learning_rate * weight_gradient[i]\n",
    "                self.biases[i] = self.biases[i] - learning_rate * bias_gradient[i]\n",
    "\n",
    "    def gradient_magnitude(self):\n",
    "        magnitude = 0\n",
    "        for i in range(len(self.weights)):\n",
    "            magnitude += np.inner(self.weights[i].flatten(), self.weights[i].flatten())\n",
    "            magnitude += np.inner(self.biases[i].flatten(), self.biases[i].flatten())\n",
    "        return magnitude ** 0.5\n",
    "\n",
    "    #return model accuracy\n",
    "    def test(self, test_data, test_labels):\n",
    "        num_correct = 0\n",
    "        for i in range(len(test_data)):\n",
    "            inference = self.make_inference(test_data[i])\n",
    "            if inference == test_labels[i]:\n",
    "                num_correct += 1\n",
    "        return num_correct/len(test_data)\n",
    "    \n",
    "    def make_inference(self, input):\n",
    "        output_layer = self.compute_output(self.get_input_layer(input))\n",
    "        return np.argmax(output_layer)\n",
    "    \n",
    "    def get_input_layer(self, input):\n",
    "        return np.array(input).flatten().reshape(-1, 1)\n",
    "    \n",
    "    def save(self, filename):\n",
    "        model = [self.weights, self.biases]\n",
    "        with open(filename, 'wb') as file:\n",
    "            pickle.dump(model, file)\n",
    "\n",
    "    def load(self, filename):\n",
    "        with open(filename, 'rb') as file:\n",
    "            model = pickle.load(file)\n",
    "            self.weights = model[0]\n",
    "            self.biases = model[1]\n",
    "\n",
    "    def train(self, train_data, train_labels, validate_data, validate_labels, learning_rate, batch_size, max_epochs, patience, filename):\n",
    "        best_validation_loss = float('inf')\n",
    "        epochs_without_improvement = 0\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "            indices = list(range(len(train_data)))\n",
    "            random.shuffle(indices)\n",
    "            shuffled_train_data = [train_data[i] for i in indices]\n",
    "            shuffled_train_labels = [train_labels[i] for i in indices]\n",
    "\n",
    "            for i in range(0, len(shuffled_train_data), batch_size):\n",
    "                batch_data = shuffled_train_data[i : i + batch_size]\n",
    "                batch_labels = shuffled_train_labels[i : i + batch_size]\n",
    "\n",
    "                weight_gradient, bias_gradient = self.batch_gradient(batch_data, batch_labels)\n",
    "                for j in range(len(self.weights)):\n",
    "                    self.weights[j] = self.weights[j] - learning_rate * weight_gradient[j]\n",
    "                    self.biases[j] = self.biases[j] - learning_rate * bias_gradient[j]\n",
    "\n",
    "            validation_loss = self.total_cost(validate_data, validate_labels)\n",
    "\n",
    "            if validation_loss < best_validation_loss:\n",
    "                best_validation_loss = validation_loss\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch + 1} with validation loss {validation_loss:.4f}\")\n",
    "                    break\n",
    "\n",
    "            train_loss = self.total_cost(train_data, train_labels)\n",
    "            print(f\"Epoch {epoch + 1}/{max_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {validation_loss:.4f}\")\n",
    "\n",
    "        print(\"Training complete.\")\n",
    "        self.save(filename)\n",
    "\n",
    "\n",
    "    def summarize(self):\n",
    "        \"\"\"Print a summary of the network's dimensions and parameter shapes.\"\"\"\n",
    "        print(\"Neural Network Summary:\")\n",
    "        print(f\"Input dimension: {self.input_dim}\")\n",
    "        print(f\"Hidden layers: {self.hidden_layers}\")\n",
    "        print(f\"Output dimension: {self.output_dim}\")\n",
    "        print(f\"Initialization method: {self.initialization}\\n\")\n",
    "        for idx, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
    "            print(f\"Layer {idx + 1} weights shape: {w.shape}\")\n",
    "            print(f\"Layer {idx + 1} biases shape: {b.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(input_dim=784, hidden_layers=[16, 16], output_dim=10, initialization=\"xavier\", activation=\"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6n/7l19skx97d53w0nbtz56v7rw0000gn/T/ipykernel_65212/1612928832.py:58: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.139836505641178"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.total_cost(x_validate, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6n/7l19skx97d53w0nbtz56v7rw0000gn/T/ipykernel_65212/3011521506.py:57: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8956"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.test(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6n/7l19skx97d53w0nbtz56v7rw0000gn/T/ipykernel_65212/1612928832.py:58: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.2841, Validation Loss: 0.2953\n",
      "Epoch 2/100, Train Loss: 0.2841, Validation Loss: 0.2953\n",
      "Epoch 3/100, Train Loss: 0.2841, Validation Loss: 0.2953\n",
      "Epoch 4/100, Train Loss: 0.2841, Validation Loss: 0.2953\n",
      "Epoch 5/100, Train Loss: 0.2841, Validation Loss: 0.2953\n",
      "Epoch 6/100, Train Loss: 0.2841, Validation Loss: 0.2953\n",
      "Epoch 7/100, Train Loss: 0.2841, Validation Loss: 0.2953\n",
      "Epoch 8/100, Train Loss: 0.2841, Validation Loss: 0.2953\n",
      "Epoch 9/100, Train Loss: 0.2841, Validation Loss: 0.2953\n",
      "Epoch 10/100, Train Loss: 0.2841, Validation Loss: 0.2952\n",
      "Epoch 11/100, Train Loss: 0.2841, Validation Loss: 0.2952\n",
      "Epoch 12/100, Train Loss: 0.2841, Validation Loss: 0.2952\n",
      "Epoch 13/100, Train Loss: 0.2840, Validation Loss: 0.2952\n",
      "Epoch 14/100, Train Loss: 0.2840, Validation Loss: 0.2952\n",
      "Epoch 15/100, Train Loss: 0.2840, Validation Loss: 0.2952\n",
      "Epoch 16/100, Train Loss: 0.2840, Validation Loss: 0.2952\n",
      "Epoch 17/100, Train Loss: 0.2840, Validation Loss: 0.2952\n",
      "Epoch 18/100, Train Loss: 0.2840, Validation Loss: 0.2952\n",
      "Epoch 19/100, Train Loss: 0.2840, Validation Loss: 0.2952\n",
      "Epoch 20/100, Train Loss: 0.2840, Validation Loss: 0.2952\n",
      "Early stopping at epoch 21 with validation loss 0.2952\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nn.train(x_train, y_train, x_validate, y_validate, learning_rate=0.001, batch_size=1000, max_epochs=100, patience=5, filename=\"model.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
